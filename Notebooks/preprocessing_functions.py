# -*- coding: utf-8 -*-
"""preprocessing_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13X3Zp2aHzBcOqV-IV0WVNALPXDcLGKT6
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile preprocessing_functions.py

import pandas as pd
import numpy as np

"""#split numerical and categorical features"""

def split_num_cat(df):

  """
    Takes a DataFrame as input and returns two lists.

    Args:
        df (pd.DataFrame): Input pandas DataFrame.

    Returns:
        nums_list, cat_list : Two lists thats holds numeical and catiogercal columns.
    """
  #decribe method return numerical columns only
  nums_list = list(df.describe().columns)
  #convert to list to be apply to subtract numerical columns from all columns
  cat_list = list(set(df.columns) - set(nums_list))

  return nums_list,cat_list

"""#heatmap with correlation"""

import matplotlib.pyplot as plt

def heatmap_with_corr(df):
  """
    Generates a heatmap showing the correlation matrix of a DataFrame.

    Args:
        df (pd.DataFrame): Input pandas DataFrame.

    Returns:
        None
    """
  import seaborn as sns
  # Calculate correlation matrix
  corr = df[df.describe().columns].corr()

  # Create the heatmap
  plt.subplots(figsize = (20, 15))
  sns.heatmap(corr,
           xticklabels = corr.columns.values,
           yticklabels = corr.columns.values,
           cmap = 'RdBu',
           linewidth = 0.1)
  plt.title("Correlation Heatmap", fontsize=20)

"""#null columns"""

def null_columns(df):
  """
    Identifies columns with null values in a DataFrame and categorizes them by type.

    Args:
        df (pd.DataFrame): Input pandas DataFrame.

    Returns:
        num_nulls (pd.DataFrame): Numerical columns with null values.
        cat_nulls (pd.DataFrame): Categorical columns with null values.
    """
  # Count null values per column
  nuls = df.isna().sum()
  nuls = nuls.reset_index()
  nuls = nuls.rename(columns = { 'index' : "Columns_names", 0 : "Nulls_Count"})

  # Sort by null count in descending order
  nuls = nuls.sort_values(by="Nulls_Count", ascending=False)

  # Filter columns with null values
  nuls = nuls[nuls['Nulls_Count'] > 0]
  print(f'data have {nuls.shape[0]} columns with null values with percentage {(nuls.shape[0] / df.shape[1]) * 100:.2f} % of all columns\n')

  # Calculate percentage of nulls
  nuls['Percentage'] = (nuls['Nulls_Count'] / df.shape[0]) * 100

  # assigns the data types of the columns in the DataFrame (df) to the Column_Type column in the nuls DataFrame.
  #(.map) matches the column names in nuls['Columns_names'] with the index of df.dtypes and retrieves the corresponding data type.
  nuls['Column_Type'] = nuls['Columns_names'].map(df.dtypes)

  # Split into categorical and numerical columns
  cat_nulls = nuls[nuls['Column_Type'] == 'object']
  num_nulls = nuls[nuls['Column_Type'] != 'object']

  return num_nulls,cat_nulls

"""#column with null percentage"""

def columns_with_null_percentage(df,percentage):
  """
    Returns a list of column names where the percentage of null values
    is greater than or equal to the specified threshold.

    Args:
        null_df (pd.DataFrame): DataFrame containing columns 'Percentage' and 'Column_Name'.
        percentage (float): Threshold for filtering columns by null percentage.

    Returns:
        list: List of column names meeting the percentage criteria.
    """
  required_columns = {"Percentage", "Columns_names"}
  if not required_columns.issubset(df.columns):
        raise ValueError(f"The input DataFrame must contain the columns: {required_columns}")

  column_list = list(df[df['Percentage'] >= percentage]['Columns_names'])

  return column_list

"""#columns_with_null_rows"""

def columns_with_null_rows(df,rows_number):
  """
    Identifies columns in a DataFrame where the number of null rows is
    less than a specified threshold.

    Args:
        df (pd.DataFrame): The input pandas DataFrame.
        rows_number (int): The threshold for the number of null rows.

    Returns:
        list: List of column names meeting the condition.
    """
  required_columns = {"Nulls_Count", "Columns_names"}
  if not required_columns.issubset(df.columns):
        raise ValueError(f"The input DataFrame must contain the columns: {required_columns}")

  column_list = list(df[df['Nulls_Count'] < rows_number]['Columns_names'])

  return column_list

"""#drop column"""

def drop_columns(df,list):
  """
    Drops specified columns from a DataFrame.

    Args:
        df (pd.DataFrame): The input pandas DataFrame.
        columns_to_drop (list): List of column names to drop.

    Returns:
        pd.DataFrame: The DataFrame with specified columns removed.
    """
  df.drop(columns = list,inplace = True)

  return df

"""#drop rows"""

def drop_rows(df,list):
  """
    Drops rows from the DataFrame where any of the specified columns contain null values.

    Args:
        df (pd.DataFrame): The input pandas DataFrame.
        columns_list (list): List of column names to check for null values.

    Returns:
        pd.DataFrame: The DataFrame with rows dropped where nulls were found in the specified columns.
    """
  df.dropna(axis = 0,subset = list,inplace = True)

  return df

"""#miss forest impute"""

def miss_forest_impute(df,list):
  """
    Imputes missing values in specified numerical columns using the MissForest algorithm.

    Parameters:
    - df (pd.DataFrame): Input DataFrame.
    - columns (list): List of numerical column names to impute.

    Returns:
    - pd.DataFrame: DataFrame with imputed values in the specified columns.
    """
  from missforest import MissForest

  imputer = MissForest()
  num_df_imputed = imputer.fit_transform(df[list])
  df_imputed = pd.DataFrame(num_df_imputed, columns=list)
  return df_imputed

"""#impute categorical columns"""

def impute_categorical(df, cat_cols):
    from sklearn.ensemble import RandomForestClassifier
    # Initialize the RandomForestClassifier
    cat_imputer = RandomForestClassifier()

    for col in cat_cols:
        # Check if there are missing values in the column
        if df[col].isnull().sum() > 0:
            # Prepare the data for the RandomForest model
            df_not_null = df[df[col].notnull()]  # Rows without missing values in the target column
            df_null = df[df[col].isnull()]       # Rows with missing values in the target column

            # Train the RandomForest model
            cat_imputer.fit(df_not_null.drop(columns=cat_cols), df_not_null[col])

            # Predict and impute missing values for categorical columns
            df.loc[df[col].isnull(), col] = cat_imputer.predict(df_null.drop(columns=cat_cols))

    return df

"""#categorical_column_frequency"""

def categorical_column_frequency(df, cat_cols):
    result_dfs = []

    for col in cat_cols:
        count_df = df[col].value_counts(dropna=False).reset_index()
        count_df.columns = ['Value', 'Frequency']
        count_df['Percentage'] = (count_df['Frequency'] / len(df)) * 100
        count_df['Column_name'] = col
        result_dfs.append(count_df)

    # Concatenate all the results into a single DataFrame
    result_df = pd.concat(result_dfs, axis=0).reset_index(drop=True)
    result_df = result_df.sort_values(by="Frequency", ascending=False)
    return result_df

"""#check balance for specfic column"""

def check_balance(df, column_name):

  """
    Computes the count and percentage distribution of unique values in a specified column of a DataFrame.

    Parameters:
        df (pd.DataFrame): The DataFrame containing the data.
        column_name (str): The column name to analyze.

    Returns:
        pd.DataFrame: A DataFrame with columns:
            - 'Class': The unique values in the specified column.
            - 'Count': The count of each unique value.
            - 'Percentage': The percentage distribution of each unique value.
    """
    # Get value counts and percentages
  counts = df[column_name].value_counts()
  percentages = (counts / len(df)) * 100

  # Create and return a DataFrame with the results
  balance_df = pd.DataFrame({
      'Class': counts.index,
      'Count': counts.values,
      'Percentage': percentages.values
  })

  return balance_df

"""#high correlated features"""

def highly_correlated_features(df, target_col, threshold=0.7):
    """
    Identifies highly correlated features and determines which to drop, keeping the feature
    more correlated with the target variable.

    Parameters:
        df (pd.DataFrame): The input dataframe.
        target_col (str): The name of the target column.
        threshold (float): The correlation threshold for identifying highly correlated features.

    Returns:
        set: A set of feature names to drop.
    """
    features = df.drop(columns=[target_col])
    target = df[target_col]
    # Compute correlation matrix for the dataframe
    corr_matrix = features.corr()

    # Extract the upper triangle of the correlation matrix
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # Identify pairs of highly correlated features
    high_corr_pairs = [
        (col, row) for col in upper_triangle.columns for row in upper_triangle.index
        if abs(upper_triangle.loc[row, col]) > threshold
    ]
    print(high_corr_pairs)
    print('\n')
    # Drop redundant features
    features_to_drop = set()
    for feature1, feature2 in high_corr_pairs:
        # Compare correlations with the target
        print(feature1);print(feature2)

        corr_with_two_feat = abs(features[feature1].corr(features[feature2]))
        corr_with_target1 = abs(features[feature1].corr(target))
        corr_with_target2 = abs(features[feature2].corr(target))

        print(f'correlation between two features : {corr_with_two_feat}')
        print(f'correlation between {feature1} and target : {corr_with_target1}')
        print(f'correlation between {feature2} and target : {corr_with_target2}')

        # Keep the one with higher correlation to the target
        if corr_with_target1 >= corr_with_target2:
            print(f'{feature2} should be dropped\n')
            features_to_drop.add(feature2)
        else:
            print(f'{feature1} should be dropped\n')
            features_to_drop.add(feature1)

    return features_to_drop

"""#columns with outliers"""

def find_columns_with_outliers(df):
    """
    Identifies columns in a dataFrame that have outliers using the IQR method.

    Parameters:
    - df (pd.dataFrame): The input dataFrame.

    Returns:
    - list: A list of column names that have outliers.
    """
    columns_with_outliers = []

    for col in df:
        Q1 = np.percentile(df[col], 25, interpolation='midpoint')
        Q3 = np.percentile(df[col], 75, interpolation='midpoint')
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Check if any value is outside the bounds
        if ((df[col] < lower_bound) | (df[col] > upper_bound)).any():
            columns_with_outliers.append(col)

    return columns_with_outliers

"""#clamp outlier"""

def outliers_clamping(df,col):

    Q1 = np.percentile(df[col], 25, method='midpoint')
    Q3 = np.percentile(df[col], 75, method='midpoint')
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Use np.where to clamp values
    return np.where(df[col] > upper_bound, upper_bound,
                    np.where(df[col] < lower_bound, lower_bound, df[col]))

"""#column with skewness"""

def transform_data(df):
    from scipy.stats import skew,boxcox
    strong_skew = []
    moderate_skew = []
    weak_skew = []

    for i in df:
      skewness_value = skew(df[i], bias=False)

      if abs(skewness_value) >= 1:
          # Strong skew: Use logarithmic transformation
          strong_skew.append(i)

      elif 0.5 <= abs(skewness_value) < 1:
          # Moderate skew: Use Box-Cox
          moderate_skew.append(i)

      else:
          # Weak skew: No transformation needed or standardization
          weak_skew.append(i)


    return strong_skew,moderate_skew,weak_skew

"""#Q-Q plot"""

def Q_Q_plots(df,ncols,nrows):
  """
    Generate Q-Q plots for numeric columns in a DataFrame.

    Parameters:
    - df (pd.DataFrame): Input DataFrame containing the data.
    - ncols (int): Number of columns in the grid layout (default is 4).
    - nrows (int): Number of rows in the grid layout (optional, auto-calculated if not provided).
  """
  import statsmodels.api as sm

  fig, axes = plt.subplots(nrows = nrows, ncols = ncols, figsize=(20, 40))
  i=0
  j=0
  for col in df:
    sm.qqplot(df[col],fit = False, line='q', ax = axes[i, j])
    axes[i, j].set_title(col)
    if(j<ncols-1):
        j+=1
    else:
        i+=1
        j=0
  plt.show()

"""#kolmogorov smirnov test"""

def kolmogorov_smirnov_test(df):
  """
    Categorize columns based on their normality using the Shapiro-Wilk test.

    Parameters:
    - df: DataFrame containing the data.
    - numeric_cols: List of numeric column names to test.
    - alpha: Significance level for the Shapiro-Wilk test (default is 0.05).

    Returns:
    - feats_std_scale: List of columns suitable for standard scaling.
    - feats_min_max_scale: List of columns suitable for min-max scaling.
  """

  from scipy.stats import kstest, norm

  feats_std_scale = []
  feats_min_max_scale = []

  # Filter columns with non-zero variance
  numeric_cols = [col for col in df if df[col].std() > 0]

  for col in numeric_cols:
    stat, p = kstest(df[col], 'norm', args=(df[col].mean(), df[col].std()))
    print(f'Column: {col}\nK-S Statistic: {stat:.3f}, p-value: {p:.8f}')

    alpha = 0.05
    if p > alpha:
        print(f'{col} looks like gaussian (fail to reject H0)')
        feats_std_scale.append(col)
    else:
        print(f'{col} does not look Gaussian (reject H0)\n')
        feats_min_max_scale.append(col)

  return feats_std_scale, feats_min_max_scale

"""#min-max scalar"""

def apply_min_max_scaler(df, columns):
    """
    Applies MinMaxScaler to the specified columns of a dfFrame.

    Parameters:
    - df: dfFrame containing the original df.
    - columns: List of column names to scale using MinMaxScaler.

    Returns:
    - scaled_df: dfFrame with MinMax-scaled values for the specified columns.
    """
    from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    scaled_df = pd.DataFrame(
        scaler.fit_transform(df[columns]),
        columns=columns,
        index=df.index
    )
    return scaled_df

"""#standard scalar"""

def apply_standard_scaler(data, columns):
    """
    Applies StandardScaler to the specified columns of a DataFrame.

    Parameters:
    - data: DataFrame containing the original data.
    - columns: List of column names to scale using StandardScaler.

    Returns:
    - scaled_data: DataFrame with Standard-scaled values for the specified columns.
    """
    from sklearn.preprocessing import StandardScaler

    scaler = StandardScaler()
    scaled_data = pd.DataFrame(
        scaler.fit_transform(data[columns]),
        columns=columns,
        index=data.index
    )
    return scaled_data

"""#one hot encoding with pandas"""

def encoding_with_get_dummies(df,cat_list):
  """
    Encodes categorical columns in the DataFrame using one-hot encoding.

    Parameters:
    - df (pd.DataFrame): Input DataFrame containing data to encode.
    - cat_list (list): List of column names to be one-hot encoded.

    Returns:
    - pd.DataFrame: DataFrame with one-hot encoded columns added, and original columns removed.
    """

  # Perform one-hot encoding with pd.get_dummies
  df = pd.get_dummies(df,columns = cat_list,drop_first = False,dtype = int)

  return df

"""#one hot encoding with sklearn"""

def OHE_with_Sklearn(df,num,cat):
  """
    Performs one-hot encoding on categorical columns using scikit-learn's OneHotEncoder
    and concatenates the result with numeric columns.

    Parameters:
    - df (pd.DataFrame): Input DataFrame containing the data.
    - num (list): List of numeric column names to retain as-is.
    - cat (list): List of categorical column names to one-hot encode.

    Returns:
    - pd.DataFrame: A new DataFrame with numeric columns and one-hot encoded columns.
  """
  from sklearn.preprocessing import OneHotEncoder
  encoder = OneHotEncoder(sparse_output=False)
  encoded_data = encoder.fit_transform(df[cat])

  # Create a DataFrame with encoded columns
  encoded_columns = encoder.get_feature_names_out(cat)
  encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)

  # Concatenate original DataFrame with encoded DataFrame
  encoded_df = pd.concat([df[num], encoded_df], axis=1)

  return encoded_df